{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9ztvpycFFZ9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "  !pip install unsloth\n",
        "else:\n",
        "  # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "  !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "  !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "  !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkTGvQfwFQnW"
      },
      "outputs": [],
      "source": [
        "# 加载unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 设置模型参数\n",
        "max_seq_length = 2048 # 选择序列最大长度，unsloth内部自动支持RoPE缩放\n",
        "dtype = None # 自动检测数据类型。Tesla T4、V100上使用Float16，Ampere+架构上使用Bfloat16\n",
        "load_in_4bit = True # 使用4位量化以减少内存使用，可以设为False关闭量化\n",
        "\n",
        "# 从预训练模型加载FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/Qwen2.5-7B\",\n",
        "  max_seq_length = max_seq_length,      # 设置之前定义的最大序列长度\n",
        "  dtype = dtype,               # 设置之前定义的数据类型\n",
        "  load_in_4bit = load_in_4bit,        # 设置是否使用4位量化\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0juTLhAHAIM"
      },
      "outputs": [],
      "source": [
        "# 继续添加一个LoRA Adapters, 冻结参数只微调1%-10%的权重\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "  model,                          # 传入之前加载的基础模型\n",
        "  r = 16,                         # LoRA秩参数，值越大模型可塑性越高，建议值为8、16、32、64、128\n",
        "                                # 该参数决定了LoRA适配器的复杂度\n",
        "  target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "                                        # 指定需要应用LoRA的模块\n",
        "                                        # 这里选择了注意力机制的投影层和MLP层作为微调目标\n",
        "                                        # q_proj/k_proj/v_proj/o_proj: 注意力机制的查询/键/值/输出投影层\n",
        "                                        # gate_proj/up_proj/down_proj: MLP网络中的门控/上投影/下投影层\n",
        "  lora_alpha = 16,                # LoRA缩放参数，通常设置为与r相同或更大，控制LoRA更新对原始权重的影响程度\n",
        "  lora_dropout = 0,               # LoRA的丢弃率，0表示不使用dropout，设为0可获得最佳性能优化\n",
        "  bias = \"none\",                  # 是否训练偏置参数，\"none\"表示不训练偏置，设为\"none\"可获得最佳性能优化\n",
        "  use_gradient_checkpointing = \"unsloth\",      # 梯度检查点设置，可选值为True或\"unsloth\"，\"unsloth\"是优化版本，比标准梯度检查点更高效\n",
        "  random_state = 42,            # 随机数种子，确保实验可重复性\n",
        "  use_rslora = False,             # 是否使用秩稳定化LoRA(Rank Stabilized LoRA)，默认不启用，启用后可提高模型训练稳定性\n",
        "  loftq_config = None,            # LoftQ配置，用于低位精度量化训练，默认不启用\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edl4Ofh6IEIj"
      },
      "source": [
        "这里用的是dair-ai/emotion数据集，做一下数据清洗\n",
        "\n",
        "huggingface:dair-ai/emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfmqRHJ4ICRd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
        "eval_dataset = load_dataset(\"dair-ai/emotion\", split=\"validation\")\n",
        "\n",
        "emo_dict= {\n",
        "  0:\"sadness\",\n",
        "  1:\"joy\",\n",
        "  2:\"love\",\n",
        "  3:\"anger\",\n",
        "  4:\"fear\",\n",
        "  5:\"surprise\",\n",
        "}\n",
        "\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "There are five types: anger, fear, joy, love, sadness, and surprise. You need to determine which category the emotion of input belongs to and return.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"text\"]\n",
        "    outputs = examples[\"label\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        emotion_word = emo_dict[output]\n",
        "        text = prompt.format(input, emotion_word) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "\n",
        "# empathetic_prompt = \"\"\"\n",
        "# ### Instruction:\n",
        "# Recognize the emotion from the input. Your should follow the Response format, including a word describing the user's emotion and a reply(No more than 30 words).\n",
        "# your return must have the fomula like this: 【happy】I guess you have the nice weekend.\n",
        "# ### Input:\n",
        "# {}\n",
        "# ### Response:\n",
        "# 【{}】{}\n",
        "# \"\"\"\n",
        "# def format_empathetic_dialogues(examples):\n",
        "#   texts = []\n",
        "#   for conversation, emotion in zip(examples[\"conversations\"], examples[\"emotion\"]):\n",
        "#     if conversation[-1][\"role\"] == \"user\":\n",
        "#       continue\n",
        "#     for i in range(0, len(conversation) - 1, 2):\n",
        "#       if i+1 < len(conversation) and conversation[i][\"role\"] == \"user\" and conversation[i+1][\"role\"] == \"assistant\":\n",
        "#         user_message = conversation[i][\"content\"]\n",
        "#         assistant_message = conversation[i+1][\"content\"]\n",
        "#         formatted_text = empathetic_prompt.format(\n",
        "#             user_message,\n",
        "#             emotion,\n",
        "#             assistant_message\n",
        "#         ) + EOS_TOKEN\n",
        "#         texts.append(formatted_text)\n",
        "\n",
        "#   return {\"text\": texts}\n",
        "\n",
        "# train_dataset = train_dataset.map(format_empathetic_dialogues, batched=True, remove_columns=train_dataset.column_names)\n",
        "# eval_dataset = eval_dataset.map(format_empathetic_dialogues, batched=True, remove_columns=eval_dataset.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yWJS7vJY7vo"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[:1][\"text\"])\n",
        "print(eval_dataset[:1][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8Kg2B9wZpwN"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        # per_device_eval_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 1,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 10,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\",\n",
        "    ),\n",
        "    # args = SFTConfig(packing=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ynvt_yZUbO4Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZeM3iNjqXqT"
      },
      "outputs": [],
      "source": [
        "# state = trainer.evaluate()\n",
        "# state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4p5l4UHH3S1"
      },
      "outputs": [],
      "source": [
        "trainer_stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "# Extract loss values\n",
        "training_loss = []\n",
        "train_steps = []\n",
        "val_loss = []\n",
        "val_steps = []\n",
        "\n",
        "for log in logs:\n",
        "  if \"loss\" in log and \"eval_loss\" not in log:\n",
        "    training_loss.append(log[\"loss\"])\n",
        "    train_steps.append(log[\"step\"])\n",
        "  elif \"eval_loss\" in log:\n",
        "    val_loss.append(log[\"eval_loss\"])\n",
        "    val_steps.append(log[\"step\"])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_steps, training_loss, label=\"Train Loss\", marker=\"o\")\n",
        "plt.plot(val_steps, val_loss, label=\"Validation Loss\", marker=\"x\")\n",
        "plt.title('Cross-Entropy Loss Over Time')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.grid(True)\n",
        "plt.savefig('training_and_evaluate_loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUKdBhccPLSV"
      },
      "outputs": [],
      "source": [
        "# 保存LoRA适配器\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhTIFV5Q89o3"
      },
      "outputs": [],
      "source": [
        "!zip -r lora_model.zip lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg6N4JI2OzDf"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c2eWOq7BAqO"
      },
      "outputs": [],
      "source": [
        "# 性格识别测试\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Then there are five types: anger, fear, joy, love, sadness, and surprise. You need to determine which category the emotion of input belongs to and return.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        \"He actually stole my business. It really pissed me off！！！！\",\n",
        "        \"\", # output\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CoXt94Ozkjz"
      },
      "outputs": [],
      "source": [
        "# 对话能力测试\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a gentle and lovely voice assistant, you need to give responses based on input.\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        # \"He actually stole my business. It really pissed me off！！！！\",\n",
        "        \"so,what do you like to do in weekend?\",\n",
        "        \"\", # output\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07ztOU2Gm0U8"
      },
      "outputs": [],
      "source": [
        "# 额外测试分块功能。\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a master of imitation. You need to insert some of the following sound effects【uh,(long-break),um,(laugh),(cough),(lip-smacking),(sigh)】 into the context to make it sound more in line with the input emotion.\n",
        "EXAMPLE:context:You are intresting! emotion:joy --->  (laugh)You are intresting!\n",
        "### Input:\n",
        "context:{} emotion:{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        \"Today is so hard. I really don't want to go to work at all.\",\n",
        "        \"sad\",\n",
        "        \"\"\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-QTEEPBgkrI"
      },
      "outputs": [],
      "source": [
        "# 测试集测试\n",
        "from datasets import load_dataset\n",
        "\n",
        "eval_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
        "\n",
        "def text_dunc(input,emotion):\n",
        "  prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "  ### Instruction:\n",
        "  Then there are five types: anger, fear, joy, love, sadness, and surprise. You need to determine which category the emotion of input belongs to and return.\n",
        "\n",
        "  ### Input:\n",
        "  {}\n",
        "\n",
        "  ### Response:\n",
        "  {}\"\"\"\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "      prompt.format(\n",
        "          input,\n",
        "          \"\", # output\n",
        "      )\n",
        "  ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens=128)\n",
        "  input_length = inputs[\"input_ids\"].shape[1]\n",
        "  generated_tokens = outputs[0][input_length:]\n",
        "  generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "  # print(generated_text)\n",
        "  if generated_text.strip() == emotion:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "emo_dict= {\n",
        "  0:\"sadness\",\n",
        "  1:\"joy\",\n",
        "  2:\"love\",\n",
        "  3:\"anger\",\n",
        "  4:\"fear\",\n",
        "  5:\"surprise\",\n",
        "}\n",
        "all_example = 0\n",
        "error = 0\n",
        "correct = 0\n",
        "for data in eval_dataset:\n",
        "  all_example += 1\n",
        "  if text_dunc(data[\"text\"],emo_dict[data[\"label\"]]):\n",
        "    correct += 1\n",
        "  else:\n",
        "    error += 1\n",
        "\n",
        "print(f\"Accuracy:{correct/all_example:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsNri8HlIOdc"
      },
      "outputs": [],
      "source": [
        "# 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model_qwen2.5_7b_Q8_0\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model_qwen2.5_7b_16bit\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model_qwen2.5_7b_q4_k_m\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# multiple GGUF options\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\",\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}