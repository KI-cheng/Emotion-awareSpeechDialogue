# 6. References

[1] Pohanka, M. (2022). Progress in Biosensors for the Point-of-Care Diagnosis of COVID-19. Sensors, 22(19), 7423.

@article{pohanka2022progress,
  title={Progress in Biosensors for the Point-of-Care Diagnosis of COVID-19},
  author={Pohanka, Miroslav},
  journal={Sensors},
  volume={22},
  number={19},
  pages={7423},
  year={2022},
  publisher={MDPI}
}

[2] Geetha, A. V., Mala, T., Priyanka, D., & Uma, E. (2024). Multimodal emotion recognition with deep learning: advancements, challenges, and future directions. Information Fusion, 105, 102218.

@article{geetha2024multimodal,
  title={Multimodal emotion recognition with deep learning: advancements, challenges, and future directions},
  author={Geetha, AV and Mala, T and Priyanka, D and Uma, E},
  journal={Information Fusion},
  volume={105},
  pages={102218},
  year={2024},
  publisher={Elsevier}
}

[3] Lialin, V., Deshpande, V., & Rumshisky, A. (2023). Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647.

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

[4] Gulati, A., Qin, J., Chiu, C. C., Parmar, N., Zhang, Y., Yu, J., ... & Pang, R. (2020). Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100.

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

[5] Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33, 12449-12460.

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

[6]Gao, Z., Li, Z., Wang, J., Luo, H., Shi, X., Chen, M., ... & Zhang, S. (2023). Funasr: A fundamental end-to-end speech recognition toolkit. arXiv preprint arXiv:2305.11013.

@article{gao2023funasr,
  title={Funasr: A fundamental end-to-end speech recognition toolkit},
  author={Gao, Zhifu and Li, Zerui and Wang, Jiaming and Luo, Haoneng and Shi, Xian and Chen, Mengzhe and Li, Yabin and Zuo, Lingyun and Du, Zhihao and Xiao, Zhangyu and others},
  journal={arXiv preprint arXiv:2305.11013},
  year={2023}
}

[7] Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Skerry-Ryan, R. J., & Wu, Y. (2021). Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling. arXiv preprint arXiv:2103.14574.

@article{elias2021parallel,
  title={Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling},
  author={Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Skerry-Ryan, RJ and Wu, Yonghui},
  journal={arXiv preprint arXiv:2103.14574},
  year={2021}
}

[8] Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.

@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

[9] Liao, S., Wang, Y., Li, T., Cheng, Y., Zhang, R., Zhou, R., & Xing, Y. (2024). Fish-speech: Leveraging large language models for advanced multilingual text-to-speech synthesis. arXiv preprint arXiv:2411.01156.

@article{liao2024fish,
  title={Fish-speech: Leveraging large language models for advanced multilingual text-to-speech synthesis},
  author={Liao, Shijia and Wang, Yuxuan and Li, Tianyu and Cheng, Yifan and Zhang, Ruoyi and Zhou, Rongzhi and Xing, Yijin},
  journal={arXiv preprint arXiv:2411.01156},
  year={2024}
}


[10] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., ... & Qiu, Z. (2024). Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

[11] Saravia, E., Liu, H. C. T., Huang, Y. H., Wu, J., & Chen, Y. S. (2018). CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 conference on empirical methods in natural language processing (pp. 3687-3697).

@inproceedings{saravia2018carer,
  title={CARER: Contextualized affect representations for emotion recognition},
  author={Saravia, Elvis and Liu, Hsien-Chi Toby and Huang, Yen-Hao and Wu, Junlin and Chen, Yi-Shin},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  pages={3687--3697},
  year={2018}
}

[12] Unsloth. (n.d.). Unsloth documentation. Retrieved March 29, 2025, from https://docs.unsloth.ai/

@misc{unsloth_documentation,
  title        = {Unsloth documentation},
  author       = {{Unsloth}},
  year         = {n.d.},
  note         = {Retrieved March 29, 2025},
  howpublished = {\url{https://docs.unsloth.ai}}
}
