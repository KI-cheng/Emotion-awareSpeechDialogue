# 5. Conclusion and Improvement 

The system provides a feasible solution for locally deployed emotion-aware dialogue systems through lightweight design and efficient resource utilization. In the future, it is expected to further meet user needs by optimizing UI interface development and improving emotion recognition accuracy.

## 5.1 Current achievements

| Aspects               | Traditional systems                       | Our system                              |
|-----------------------|-------------------------------------------|-----------------------------------------|
| Resource requirements | High (High performance server required)   | Low (Supports consumer hardware)        |
| Deployment            | Cloud services                            | Support local deployment                |
| Training efficiency   | Resource intensive, slow to train         | Memory usage is reduced by 70%          |
| Deployment threshold  | High                                      | Low                                     |
| Expandability         | Low (End-to-end)                          | High (Multiple modules)                 |


- Innovative solution: We proposed a solution for a locally deployable emotion-aware dialogue system, which saves a lot of computing resources.
- Lightweight: The system has a low deployment threshold through lightweight design and is suitable for running on ordinary hardware.
- Emotion recognition: The system performs well in emotion recognition and response generation, and is easy to expand and maintain.
Modular structure: In the future, it can be further developed by optimizing and adding multimodal functional modules.

## 5.2 Future potential

Based on current research, the following directions may further improve the performance of the system.
- Improve emotion recognition accuracy: Develop more advanced algorithms using larger and more diverse data sets, especially when dealing with complex emotions (such as mixed emotions or culturally specific emotions). For example, expand the training data to cover more languages and cultural backgrounds.
- User-friendly interface development: Designing an intuitive graphical user interface (GUI), developing a voice interaction interface, and simplifying the configuration and usage process can further improve the application scenarios. It is also possible to develop a borderless application-level UI and develop a custom LLM desktop pet.
- Enhance multimodal integration: The current system already supports voice and text, and in the future, facial expressions can be integrated as input to build a more comprehensive emotion understanding framework.
